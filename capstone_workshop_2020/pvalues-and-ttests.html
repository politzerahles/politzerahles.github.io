<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "DTD/xhtml1-transitional.dtd">
<html lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html;charset=UTF-8" />
        <title>p-values and t-tests</title>
        <link rel="stylesheet" type="text/css" href="../stevetools_stylesheet.css" />
	<script type="text/javascript"
		src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>
    </head>

<body>

    <body>

		<h1>Basic introduction to inferential statistics and <em>t</em>-tests</h1>

		<h3 class="center"><u>Introduction to <em>p</em>-values</u></h3>

		<p class="centercol"><em>p</em>-values are weird, difficult to understand, stupid, commonly misinterpreted, and mostly useless. Nevertheless, in your research career, people will often ask you to show a p-value for something in your research. Thus, let's take some time to understand what p-values are and what they mean.</p>

		<h4 class="centercol">Samples and populations</h4>
		<p class="centercol">When we do research, typically we are interested in some <u>population</u>. E.g., in a study of whether a certain kind of learning method is more effective than another kind of learning method, the population might be all people in the world, all people of a certain age, all people of a certain age in Hong Kong, etc.; it depends what group we want to be able to make conclusions about.</p>

<p class="centercol">Usually it is not possible to collect data from the entire population. For example, if we want to test whether primary school students learn better through flipped classrooms or traditional classrooms, the population we are interested in is "all primary school students", but we probably do not have enough time or money to test every primary school student in the world (and it is impossible to test past and future primary school students). Or if we want to test whether the brain processes nouns differently than it processes verbs in Chinese, the population we are interested in is "all nouns and verbs in Chinese, and all people's brains", but we probably do not have enough time to do brain recordings as every person reads every noun and verb in the whole language.</p>

<p class="centercol">Therefore, in research we usually only collect data from a <u>sample</u>. For example, instead of testing flipped and traditional classrooms among all primary school students in the world, maybe we just test it among 50 students at a local school where it's convenient for us to do the test.</p>

<p class="centercol">Consider the example data below. Let's imagine I want to know if PolyU graduate students are older than HKUST graduate students. The population I am interested in, therefore, is all PolyU graduate students and all HKUST graduate students. But I don't have access to the personal records, so I can't look up all their ages; instead, I just go to each school and ask random people around campus. I don't have time to ask every graduate student at each school, so maybe I just decide to ask 10 at each school. Here are my results (these are fake data):<ul class="center"><li><strong><u>PolyU</u></strong>: 25, 24, 23, 31, 33, 25, 26, 28, 44, 30</li><li><strong><u>HKUST</u></strong>: 17, 35, 23, 25, 27, 24, 25, 25, 21, 24</li></ul></p>

<p class="centercol">If I want to know if my PolyU students are older than my HKUST students, all I need is some basic math. The average age of the PolyU students I asked is 29, and the average age of the HKUST students is 25. So my group of PolyU students is 4 years older, on average, than my group of HKUST students.</p>

<p class="centercol">However, this is only a statement about my sample. What I really want to know is whether PolyU students are older than HKUST students in the population. This random sample I got might not reflect the population well. For example, maybe PolyU students are actually younger on average, but I just happened to have bad luck and I randomly bumped into the oldest students at PolyU, or randomly bumped into the youngest students at HKUST.</p>

<p class="centercol">The bad news is, what I want to know is impossible. There is no way to know what the situation is in the population, unless I am actually able to measure the whole population. (In this case it might actually be possible, since the university might have records of all its students; someone with the right computer access might be able to get all that data. But in normal research contexts, it is usually not possible to know what the result looks like for the entire population.)</p>

<p class="centercol">Is there a risk that the random sample I got does not match the population? In reality, we usually don't know. But in this case, I made up fake data, so we can figure it out. <a href="./sampledata.xlsx">This Excel sheet</a> has the ages of 500 fake PolyU students and the ages of 500 fake HKUST students. Let's imagine, for the sake of this lesson, that that is all the students at each school. In other words, we have data from the entire population. Look at the data and calculate the average age of each group. (You should find an average age of 27 for the PolyU students, and an average age of 25 for the HKUST students; i.e., PolyU students are 2 years older than HKUST students, on average.) In this case, since I made up the data for the whole population, we know what the "right" answer is. Any time we choose a random sample of 10 PolyU students and 10 HKUST students, we should expect to find that the PolyU students are older; that is the "correct" answer (in that it matches what we know to be true in the population). But, as I mentioned above, there is a chance that maybe if we have bad luck, we might randomly get a sample that gives us the "wrong" answer (i.e., a sample where the PolyU students are younger). You can try this for yourself. Randomly select 10 PolyU students and 10 HKUST students and compare their averages. Were the PolyU students older (the "correct" answer) or younger (the "wrong" answer)? Keep trying this again and again until you randomly get a sample with a "wrong" answer. How many tries did it take you?</p>

<p class="centercol">(For me, I got a wrong answer on my second try! I tried taking random samples like this 500 times [using a computer program] and 92 of those samples [18%] got the "wrong" answer.)</p>

<p class="centercol">So, we understand that it is possible for a sample to be "wrong" (i.e., to be inconsistent with the pattern in the population). So after we do a study, we can calculate what the result is in the sample, but we don't know if this result is a "right" one or a "wrong" one. In my fake example here, we know what the "right" result is because we know what the data for the whole population looks like; in reality, you usually do not know that (if you did, you wouldn't need to do the research). How can we figure out if our sample's result was consistent or inconsistent with the population?</p>

<p class="centercol">The short answer is: we cannot. Research is never certain. This is the difference between math and statistics. Math is the science of certainty (it's all about absolute proofs); statistics is the science of uncertainty. Statistics never gives you an answer; all it does is help you quantify how uncertain you are about something. This is one of the reasons we usually don't say a research result "proves" something.</p>

<p class="centercol">So what can we do? The best solution is replication. If we do one study (e.g., collect one sample of 10 PolyU students and 10 HKUST students) then we have little idea whether that result matches the population or not. But if we repeat this study twenty times (each time getting a new sample of 10 PolyU students and 10 HKUST students), we can have a better idea. If we do the study twenty times and eighteen of them show that the sample of PolyU students is older than that sample of HKUST students, we can be fairly confident that PolyU students are probably older than HKUST students in the population.</p>

<p class="centercol">It's often not feasible to replicate the same study many times, though. Think about how much effort it takes you to do one study for your dissertation; do you really want to do it twenty times? This is sometimes possible in more controlled disciplines like physics. But in social sciences, where we often have to struggle to get people to join an experiment, it might be hard to do. And you might prefer to move on to new research rather than repeating the same experiment twenty times.</p>

<p class="centercol">The next best solution is to use large samples. The larger your sample, the more likely it is to match the pattern that is present in the population. At the most extreme: if your sample is the entire population, then it is guaranteed to match the population. Remember how earlier we tried taking samples of 10 students from each university, and I said that 18% of those samples got "wrong" results? Try doing that again with samples of 30 students from each university. Is it harder to get a wrong result? How many tries did you have to make before you got a wrong result (or got tired and gave up)?</p>

<p class="centercol">(I just tried it 500 times again, and this time only 7% of those samples got wrong results.)</p>

<p class="centercol">So, the best things to do for research are to replicate studies and to use large samples. But sometimes this is not enough; we may not have time to run many replications or run a sufficiently large sample. In this case, we still want to have some idea of how much we should trust our result. This is what people typically use p-values for.</p>
<h4 class="centercol"><em>p</em>-values</h4>
<p class="centercol">Remember that above I randomly selected 10 PolyU students and 10 HKUST students and found that, on average, the PolyU students were 4 years older (actually 4.3 years older, to be exact) than the HKUST students. If I only collected that sample and I did not have access to the data from the whole population, I would have no way of knowing if this result matches the population (i.e., PolyU students really are older than HKUST students, on average) or if this result does not match the population (i.e., PolyU students are not older than HKUST students on average).</p>

<p class="centercol">People often try to help deal with this question using a p-value. p-values are weird things. A p-value tells us how likely we would have been to find such a big difference (i.e., PolyU students being 4.3 years older, or more, than HKUST students) if there were really no difference in the population.</p>

<p class="centercol">In other words, imagine that there is really no overall difference between PolyU and HKUST students in the population. There may be some variation (PolyU has some older students and some younger students, and so does HKUST), but the averages are equal. If that were the case, then every time we take a random sample of 10 PolyU and 10 HKUST students, there would still be a chance that the PolyU student sample is older, just because we might have randomly picked relatively old PolyU students and relatively young HKUST students.</p>

<p class="centercol">Let's see an example. I have done a computer simulation by creating new fake populations of PolyU and HKUST students, and this time I made them so that they have the same average age. Then, I randomly took 10 PolyU students and 10 HKUST students and compared their ages. I repeated this 5,000 times. I've shown the results in the graph below. The height of the curve indicates how many tests showed a difference of that amount. In other words, in most of my samples, the difference between PolyU and HKUST is near zero (that's where the curve is highest). This is what we should expect, since the realities really do have the same average age. But sometimes, because of bad luck, we happen to pick 10 pretty old PolyU students and 10 pretty young HKUST students. So you can see that in some of my samples, PolyU students are more than 5 years older than HKUST students, or as much as 10 years younger. These samples are not as common (you can see that the curve is pretty low near those edges) but they can still happen.</p>


<center><img src="distgraph.png" alt="A normal curve centered near zero, with tails extending to about -10 and +5." width="779" height="808" /></center>

<p class="centercol">But what about my "real" experiment, where the PolyU students were 4 years older than HKUST students? If there were really no difference between PolyU and HKUST students in the population, would it have been easy to get a difference this big in our sample? In the graph below, I have shaded in the part of the distribution that represents the samples where PolyU students were 4.3 years (or more) older than HKUST students.</p>

<center><img src="distgraph2.png" alt="A normal curve centered near zero, with tails extending to about -10 and +5. The positive tail from about +4 onwards is shaded red." width="764" height="799" /></center>

<p class="centercol">In this example, that area corresponds to 4% of the distribution. In other words, I took 5000 samples, and 4% of those samples got a PolyU-HKUST difference of 4.3 years or more.</p>

<p class="centercol">This number, 4%, is a p-value (although it's usually expressed as a proportion [.04] rather than a percentage [4%]). What it means is: if there were no difference in the population, then there is a 4% chance that I would have found a result as big (or bigger) than what I found in my real experiment.</p>

<p class="centercol">Be very careful interpreting what this means. People often want to say that a p-value is the likelihood that your result was wrong (i.e., the likelihood that your result does not match the population, or that your result was "due to random chance"). This is INCORRECT. A p-value tells how likely you would have been to get this result if there was no difference in the population. It does not tell you how likely it is that there was no difference in the population if you got this result. The likelihood of X, given Y, is not the same as the likelihood of Y, given X. For example, if something is a dog, then you know that it's a mammal. But if something is a mammal, you don't know that it's a dog. Saying "if I know this animal is a mammal, then there's a 5% chance that it's a dog" is not the same as saying "if I know this animal is a dog, then there's a 5% chance that it's a mammal". Likewise, saying "if there were no difference in the population, there's a 4% chance that I could have gotten this result" is not the same as saying "if I got this result, there's a 4% chance that there's no difference in the population".</p>

<p class="centercol">This is a serious problem. Usually we are interested in taking our data and making a conclusion about the population. But p-values cannot do that. p-values take an assumption about the population (i.e., what the population would look like if there were no difference between groups) and make a conclusion about your data (i.e., if there were no difference between groups, it would be hard to get a sample of data like this).</p>

<p class="centercol">The logic people use with p-values is usually something like the following: "if there were no difference between PolyU and HKUST in the population, then there's not a good chance that my samples would have a 4.3-year age difference. So I prefer to conclude that there is a difference between PolyU and HKUST." This logic, however, is also problematic. Showing that one situation (no difference between PolyU and HKUST) is unlikely does not necessarily prove that another situation (PolyU students being 4.3 years old) is likely. Maybe the new conclusion you make is even less likely than the other one. For example: we know that if a given person is Chinese, it is not very likely that that person is Ai Weiwei (there are over a billion Chinese people, and only one Ai Weiwei!). Now imagine we see a person, and that person is Ai Weiwei. As we said before, if we pick any random Chinese person, it's not likely that the person is Ai Weiwei. So should we conclude that this person we just saw is not Chinese? No! Likewise, even if we can say "if there were no population difference between PolyU and HKUST, there it's unlikely there would be a 4.3-year age difference in my sample", that does not necessarily give us license to conclude "then there is probably a 4.3-year age difference in the population".</p>

<p class="centercol">Are you feeling confused yet? If so, good! p-values are confusing. If you use them, you should appreciate how weird and confusing they are, and think carefully about what conclusions you make about p-values, rather than being overconfident about what you can say based on a p-value. People who make strong and confident claims based on their research's p-value are usually people who don't actually understand what p-values mean.</p>

<p class="centercol">So, overall, p-values have a lot of problems. And the ways that most people interpret p-values are incorrect. If you use p-values, be very careful what you say about them.</p>

<p class="centercol">Nevertheless, in your research career you will probably need to use p-values sometimes. Many people who have power over your future (i.e., your professors, and the reviewers of papers that you write) often ask you to show p-values. Therefore, you have to know ways to calculate them. The rest of this page will discuss some ways to do that using something called a <em>t</em>-test. First, though, let's test how well you understand <em>p</em>-values.</p>

<h3 class="center"><u>Self-test on <em>p</em>-values</u></h3>

<p class="centercol">Read <a href="https://www.polyu.edu.hk/cbs/sjpolit/pubs/Politzer-AhlesChen_significance.pdf" target="_blank" rel="noreferrer noopener">Politzer-Ahles &amp; Chen (2019)</a> and at least one of the references cited therein.*</p>
<p class="centercol">Once you're finished, try answering the true/false questions below.</p>
<p class="centercol">*Other useful (but longer) readings about <em>p</em>-values, and statistical testing logic in general, include <a href="http://library.mpib-berlin.mpg.de/ft/gg/GG_Mindless_2004.pdf" target="_blank" rel="noreferrer noopener">Gigerenzer (2004)</a> and <a href="https://errorstatistics.files.wordpress.com/2015/05/wagenmakers_2007_pvalueproblems.pdf" target="_blank" rel="noreferrer noopener">Wagenmakers (2007)</a>.</p>

<p class="centercol">For this test, imagine the following made-up scenario:</p>
<p class="centercol"><em>I did a research project in which I taught Chinese tones in two different ways to two groups of students. One group (the "control" group) was taught using a traditional method. Another group (the "experimental" group) was taught using a new method I created based on some learning theory. At the beginning and end of the semester I tested each group in a test of their tone recognition. I found that the control group improved 27 points on their tone recognition from the beginning to the end of the semester, whereas the experimental group improved 40.7 points. In other words, the experimental group's improvement was 13.7 points higher than the control group. I used a statistical test to compare the improvement between the two groups, and found that the difference in improvement was statistically significant, p=.022.</em></p>
<p class="centercol">Now look at the six statements below about the results of this study. For each statement, decide whether the statement is true or false, based on the description above. The answers are given at the very bottom of this page.</p>
<ol class="center">
<li>I proved that the null hypothesis (no difference in learning outcomes between the two teaching methods) is false.</li>
<li>I showed that the null hypothesis (no difference in learning outcomes between the two teaching methods) is unlikely.</li>
<li>I proved that there is a difference between the two teaching methods.</li>
<li>It is likely that there is a difference between the two teaching methods.</li>
<li>The null hypothesis is that both teaching methods lead to the same learning outcomes; if I reject this null hypothesis (i.e., if I conclude that the teaching methods lead to different learning outcomes), there is a 2.2% chance that I will have made a mistake.</li>
<li>The null hypothesis is that both teaching methods lead to the same learning outcomes; based on the results of my study, there is only a 2.2% chance that this hypothesis is correct. (In other words, there is a 97.8% chance that the teaching methods lead to different learning outcomes.)</li>
</ol>


<h3 class="center"><u>Doing a <em>t</em>-test by hand</u></h3>

<p class="centercol">A p-value is calculated with a hypothesis test (more formally called a null hypothesis statistical test). The example you read about in Task 1 of this module was one way of carrying out a hypothesis test: I created a fake population where there was no difference between the groups, and then looked to see how many samples from that population would show a big difference.</p>

<p class="centercol">It often is not feasible to create a fake population like this (for example, we might not known how much variation in age there should be in the population, so we wouldn't know how to make up the fake population). Instead, people usually use a math formula to approximate this kind of test. Specifically, they use a math formula to calculate a number (called a test statistic) which falls somewhere on a known distribution (unlike my example in the earlier task, where I had to make up a distribution by simulating tests on fake data 5000 times) and then get the p-value by seeing where in the distribution the test statistic falls.</p>

<p class="centercol">One of the simplest and most common examples of a statistical procedure like this is the t-test. Imagine that I have ten students, and I compare each student's score on a math test and reading test; I want to see if the students do better at reading than math. For each student, I can calculate the reading test score minus the math test score to see how much better they did at reading. Here is a set of scores:</p>

<center>5, 17, -6, 3, 12, -11, 8, 13, 10, -2</center>

<p class="centercol">We can see that, on average, students did better on reading than they did on math (the average of these values is 4.9, meaning they scored an average of 4.9 points higher on reading than they did on math). But some students actually did worse on reading. And, as we know, the results from this sample might not match the results of the population. We want to do a statistical test to help us decide whether to conclude that, in the population, this reading-minus-math difference is likely to be bigger than zero (if reading and math scores are the same in the population, the difference between them would be zero). Keep in mind that calculating a p-value does not actually answer this question, for the reasons we discussed in the previous tasks; nonetheless, you may be expected to calculate a p-value anyway.</p>

<p class="centercol">To get a p-value, we first calculate a t statistic using the below formula:</p>

<center>\(t=\frac{\bar{x}}{^s/_\sqrt{N}}\)</center>

<p class="centercol">\(\bar{x}\) refers the average of the results (4.9), s to the sample standard deviation (a measure of how much the results vary across people; you can calculate this in Excel or other statistical software, or by hand), and N to the number of participants. If I plug all the numbers into the equation I get the following:</p>

<center>\(t=\frac{4.9}{^{8.94986}/_\sqrt{10}}=1.73133\)</center>

<p class="centercol">So, the t-statistic for these data is 1.7313. The next step is to know what p-value this corresponds to. In the past we would do this by looking up the value from a table, but nowadays most statistical software calculates this automatically. In this case, the p-value is .05872, meaning that if there were no difference between reading and writing scores in the population, there is a 5.872% chance that we might have observed a t-value of 1.73133 difference in this sample. (A good rule of thumb is that if your study has a large enough sample, t-values above 2 or below -2 will tend to have p-values below .05. If your study has a small sample [e.g. less than about 30 participants or items] this rule of thumb will start to break down.)</p>

<p class="centercol">There are many more nuances to how to do a t-test. First of all, the formula for calculating a t-value is slightly different if you are just examining one average (as in our present example, seeing if the number is different from zero) or if we are examining two averages that come from two different groups of people (as in the PolyU/HKUST example discussed above). And the way you calculate a p-value from a t-statistic depends on your previous predictions (in this case, I had predicted beforehand that reading scores would be higher; if I had not made that prediction, the p-value for this same t-statistic would be different). If you plan to use a <em>t</em>-test in your own research, you will need to read more about it to make sure you handle these issues correctly; some of these issues will be addressed in the next section.</p>

<p class="centercol">Nevertheless, the formula for the t-statistic is a useful formula to know, because it gracefully illustrates all the things that are important when you design research. To get a statistically significant result (i.e., a small p-value), you want to get a high t-statistic (the higher the t-statistic is, the smaller its corresponding p-value will be, if all else is held constant). If you look at the formula for the t-statistic above, and do some basic math, you should see that there are three things you could increase to make t bigger:
<ol class="center"><li>If \(\bar{x}\) (the size of the effect) is bigger, t will be bigger;</li>
    <li>If <em>s</em> (the variation across participants) is smaller, t will be bigger;</li>
    <li>If <em>N</em> (the number of participants) is bigger, t will be bigger.</li></ol></p>

<p class="centercol">Therefore, the t formula is a perfect summary of the three things you can do in your study to maximize the chance of finding a significant effect. If you try to find bigger effects (by doing whatever you can to make the difference as big as possible; e.g., choosing tests that accentuate the difference between math and reading, as opposed to choosing a "math" test that still requires a lot of reading), minimize the variation across participants (e.g., by trying to test people under as similar circumstances as possible, rather than e.g. testing some students at night and some in the morning), and find as many volunteers as you can, you can increase your chance of finding a significant effect.</p>

<p class="centercol">Test yourself in calculating a <em>t</em>-value by hand. 
Imagine I test students at both the beginning and end of the semester to see if they improve. At the end of the semester, I take their end-of-semester score minus their beginning-of-semester score to see how much improvement they had. Now imagine my class has 1000 students but I could only perform these tests for 20 randomly selected students. Therefore, maybe the university wants to know if the improvement I observed in just 20 students is statistically significant (even though a statistical significance test doesn't answer that question).</p>

<p class="centercol">Below are the 20 improvement scores for the students I tested. Calculate the t-statistic for checking if these improvement scores are significantly greater than zero. The answer is at the end of the page.</p>

<center>{-5, 9, -4, 8, 9, -1, 12, 4, 11, 14, 1, 14, 15, -2, -4, 4, 2, 5, 14, 10}</center>


<h3 class="center"><u>Other types of <em>t</em>-test</u></h3>
<p class="centercol">What we discussed in the previous section was called a <em>one-sample t-test</em>. We took one sample of numbers, and compared the mean of that sample to some other number. That number is usually zero: e.g., in our t-test, we checked to see whether the average improvement for our sample of 20 students was greater than zero.</p>
<p class="centercol">Another kind of t-test is the <em>paired samples t-test </em>(also sometimes called a <em>dependent samples t-test</em>). This happens when you have two sets of numbers (e.g., pre-test scores and post-test scores) and you compare their means (e.g., to see if the average post-test score is higher than the average pre-test score). For it to be a <em>paired samples</em> t-test, the scores have to come in natural pairs: for example, a pre-test score and a post-test score from the same person makes a pair. If your pre-test scores and post-test scores are from different people, you can't use a paired samples t-test. I won't talk much more about the paired samples t-test because it's actually literally the same thing as a one-sample t-test. Checking whether someone's post-test score is higher than their pre-test score is exactly the same thing as checking whether someone's {post-test minus pre-test} difference is higher than zero. (For example, imagine someone's pre-test score is 63 and their post-test score is 85. Mathematically, you could express this as "85 &gt; 63", or you could express it as "85-63 &gt; 0" ; these are the same.) Thus, every paired samples t-test could be restated as a one-sample t-test. In fact, the formula for a paired samples t-test is essentially the same as the formula for a one-sample t-test.</p>
<p class="centercol">Last, there is an <em>independent samples t-test</em>. This is what you use when you want to compare two samples of data, and the data do <em>not</em> come in natural pairs. For example, if I want to see if PolyU students are taller than HKU students, I would need to use an independent samples t-test, because my measurements of PolyU students come from different people than my measurements of HKU students. The formula for an independent samples t-test follows the same logic as the one-sample t-test formula you learned earlier (its key parts are still the size of the group difference, the amount of variance, and the size of the sample), but some of the details are different. I usually just calculate it using statistical software.</p>
<p class="centercol">When you use statistical software for making an independent samples t-test, you will have to indicate whether your samples have [approximately] <em>equal variance</em> or [approximately] <em>unequal variance</em>. (For example, imagine if you want to compare 20 cats and 20 dogs to see whether cats or dogs are heavier. Cats' weight doesn't vary a lot; all housecats are more or less the same size. Dogs' weight varies a lot: there are some tiny dogs and some big dogs. Thus, these two groups have <em>unequal variance</em>.) In general, it's best to just always use the unequal variance test (in fact, some statistical software, like R, does this by default).</p>
<p class="centercol">Additionally, when you use statistical software for any t-test (independent or one-sample), you will have to indicate whether you want a two-tailed test or a one-tailed test. A two-tailed test is what you use when you have no directional prediction: Group A might be higher than Group B or Group B might be higher than Group A, you have no idea, you just want to see if the groups are different in either direction. I think this situation is rare, but people still often use the two-tailed test anyway. A one-tailed test is what you use when you have a directional prediction: e.g., you expect that Group A is <em>higher</em> than Group B, and you have no interest in testing whether Group A is <em>lower</em> than Group B. If you have a directional prediction, an appropriate one-tailed test has a better chance of finding a significant <em>p</em>-value. However, you have to be careful that you're looking in the right tail (if you expect Group A to be higher than Group B but you accidentally put the two groups into the Excel formula in the wrong order, Excel might totally miss the effect). Understanding all the details of how this works is beyond the scope of the present class (but you can learn more about it in a statistics class). My recommendation is: if you are confident with what you're doing, use one-tailed tests when appropriate to have a better chance of finding the effects you're looking for; but if you're not confident or not sure what you're doing, just use a two-tailed test.</p>
<p class="centercol">To learn how to do a t-test in Excel, see <a alt="" href="./Rasinger_ttests.pdf" target="_blank">Rasinger_ttests.pdf</a>. (The instructions here assume that you have activated the "Data Analysis Toolkit" in Excel. This is not activated by default, but can be activated with a few simple steps; it should be easy to Google for instructions on how. I usually don't use this toolkit, I usually just directly use the =T.TEST() function, but that function only reports the <em>p</em>-value, whereas the Data Analysis Toolkit also reports the <em>t </em>statistic and other relevant numbers that you might need.)</p>
<p class="centercol">To learn how to do a t-test in R, see <a alt="" href="./t-test in R.pdf" target="_blank">t-test in R.pdf</a> </p>
<p class="centercol">Practice doing a t-test on your own. Recall the first activity in this page, where I listed the ages of some [imaginary] PolyU and UST graduate students:
<ol class="center"><li><strong><u>PolyU</u></strong>: 25, 24, 23, 31, 33, 25, 26, 28, 44, 30</li><li><strong><u>HKUST</u></strong>: 17, 35, 23, 25, 27, 24, 25, 25, 21, 24</li></ol></p>
<p class="centercol">Do a t-test to compare these two groups. You'll have to decide which kind of t-test to use. Report the p-value that you get from this test. The answer is listed at the end of this page.</p>

<h3 class="center"><u>Choosing the right test</u></h3>
<p class="centercol">In this module we have discussed <em>t</em>-tests. You probably have noticed that there are other modules about other kinds of tests (such as regression and analysis of variance), and you may have other of still other kinds of tests that are not addressed in this subject (such as chi-squared tests). How do you decide what test to do? Why did we use <em>t-</em>tests, instead of other ones, for the examples in this module?</p>
<p class="centercol">To understand how to choose a test, you have to understand the variables in your research design. A variable is some thing that you measure or manipulate in your study. Things that you measure might be stuff like people's proficiency, test scores, brain responses, age, etc. Things that you manipulate might be things like which kind of training each person gets, which kind of sentence they read in an experiment, etc. There are two important distinctions to consider with your variables:</p>
<h4 class="centercol">Independent vs. dependent</h4>
<p class="centercol">In most research you are interested in causes and effects: e.g., you might predict that giving people a different kind of training<em> </em>will cause them to learn grammar better, or showing people a different kind of sentence will cause their brain to react a certain way, or having higher motivation will cause people to learn tone better, or whatever. (Note that statistical methods usually cannot prove cause and effect in observational studies -- see the "Types of research designs" module. Here I'm oversimplifying.) Generally, the independent variable (???) is the thing that you think causes something, and the dependent variable (???) is the effect that you want to measure. Another way of looking at it is that, in experiments, the independent variable is the thing you manipulate (either directly, by, e.g., assigning people to different training groups; or indirectly, by, e.g., making sure that your student sample has a good mix of high-proficiency, low-proficiency, and medium-proficiency students), whereas the dependent variable is the thing that you measure. In the example we discussed previously about comparing the age of PolyU students and UST students, "school" is the independent variable and "age" is the dependent variable -- we "manipulated" the school variable by intentionally measuring people from two different schools, whereas we just passively observe what their ages are.<em></em></p>
<h4 class="centercol">Measurement scales</h4>
<p class="centercol">Variables roughly fall into three types of measurement scale. Some variables are lumped into categories, with no inherent order. For example, in a study that compares "experimental group" vs "control group", or a study that compares "French speakers" vs. "Korean speakers", there is no inherent order; there's no particular reason to call French speakers "Group 1" and Korean speakers "Group 2" or vice versa. These are <em>nominal</em> variables. (Sometimes they are also called <em>categorical</em> variables; in R, they are called <em>factors</em>.)<em> </em></p>
<p class="centercol">Some variables do have an inherent order, but the distances between each level are not the same. For example, imagine you have people take a survey (such as a evaluation you all take at the end of every class) and rate how satisfied they were, with choices such as "Very dissatisfied", "Somewhat dissatisfied", "Neutral", "Somewhat satisfied", and "Very satisfied". Or imagine you give participants a language background questionnaire and ask how often they use English, with choices like "Never", "Sometimes", "Often", and "Always". These have a clear order ("never" is less than "sometimes", "sometimes" is less than "often", etc.). But the distance between levels may not be the same: maybe "often" is a lot more than "sometimes", but "always" is only a little bit more than "often". This is called an <em>ordinal</em> variable.</p>
<p class="centercol">Finally, some variables work like true numbers. They have an order, and the distances between each number are consistent. For example, if you measure how tall someone is, or if you measure how quickly someone can finish reading a sentence, the results you get are actual meaningful numbers. This is called an <em>interval</em> variable (sometimes called a <em>numeric</em> or <em>continuous</em> variable).</p>
<h4 class="centercol">Putting it all together</h4>
<p class="centercol">The reason this stuff is important is because it determines what kinds of test you use. Different tests are suited for different kinds of variables. For example, a <em>t</em>-test works great when you have a nominal independent variable (with only two levels) and a numeric dependent variable; you can review the examples from throughout this module and see that they all fit this criteria. If you have a continuous independent variable and a continuous dependent variable, <em>t</em>-test will be no good. Likewise if you have a nominal independent variable and a nominal dependent variable. Likewise if you have a continuous dependent variable and a nominal independent variable with more than two levels (e.g., if your research project has three groups instead of two). For situations like that, you'll need different tests. As you've seen in previous tasks, the nature of the relationships between groups (e.g., paired vs. independent) will also influence what kind of test you need. <a href="https://stats.idre.ucla.edu/other/mult-pkg/whatstat/" target="_blank" rel="noreferrer noopener">https://stats.idre.ucla.edu/other/mult-pkg/whatstat/</a> is a great resource for seeing what kind of test you need in any situation. </p>

<p class="centercol">To reflect on what you've learned here, try to think of one kind of research situation that an independent samples t-test would be appropriate for, one kind of research situation that a paired samples (or one-sample) t-test would be appropriate for, and one kind of research situation that would need some other test instead of a t-test.</p>
<hr>
<p class="centercol"><em>Answers to self-test on p-values:</em> All six of the statements are false. If you got any of these wrong (i.e., if you thought some were true), review the reading to understand why all of them are false. In particular, keep this key fact in mind: <em>p</em>-values are statements about the likelihood of certain <em>data</em>, not statements about the likelihood of any hypothesis.</p>
<p class="centercol"><em>Answer to calculating t-value by hand:</em> 3.866215</p>
<p class="centercol"><em>Answer to t-test problem:</em> You will get p=0.096925231 if you do a two-tailed test, or p=0.048462616 if you do a one-tailed test.Whether you use a two-tailed or a one-tailed test depends on your original hypothesis. Note that both of these p-values come from the "unequal variance" t-test; the "equal variance" t-test never needs to be used.</p>

<hr>
	<p class="mini">by <a href="..">Stephen Politzer-Ahles</a>. Last modified on 2020-10-23.</p>



</body>